{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Urban sound dataset"
      ],
      "metadata": {
        "id": "xWrBLv45I9n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhbV6sN6JCCn",
        "outputId": "e9baefbe-63f9-4a91-ca2d-e4bdc1c5cecc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "metadata": {
        "id": "F_WB4grRJB_w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKJTGdSwJB85",
        "outputId": "374fe380-ec43-4f3d-ccaf-199c308d9fed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ],
      "metadata": {
        "id": "LJXoI9CNJZwU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/chrisfilo/urbansound8k'\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI2ZHQ7XJZtN",
        "outputId": "8c222da0-cd28-495d-e447-a1520712d10e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: saimislam\n",
            "Your Kaggle Key: ··········\n",
            "Downloading urbansound8k.zip to ./urbansound8k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.61G/5.61G [01:11<00:00, 84.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset Class"
      ],
      "metadata": {
        "id": "yEyJirFlKQyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "sxAE1DiL5lR1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UrbanSoundDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 annotations_file,\n",
        "                 audio_dir,\n",
        "                 transformation,\n",
        "                 target_sample_rate,\n",
        "                 num_samples,\n",
        "                 device):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.device = device\n",
        "        self.transformation = transformation.to(self.device)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = signal.to(self.device)\n",
        "        signal = self._resample_if_necessary(signal, sr).to(self.device)\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "        signal = self.transformation(signal)\n",
        "        return signal, label\n",
        "\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length_signal\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
        "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
        "            index, 0])\n",
        "        return path\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 6]"
      ],
      "metadata": {
        "id": "qJvOL64gGntq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQr87BTbM8zm",
        "outputId": "e76fd575-ebe4-4093-dd82-6750d360188a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply all data preprocess"
      ],
      "metadata": {
        "id": "mP2h6ltEOSoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATIONS_FILE = \"urbansound8k/UrbanSound8K.csv\"\n",
        "AUDIO_DIR = \"urbansound8k\"\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mel_spectrogram,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device)\n",
        "print(f\"There are {len(usd)} samples in the dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2H2OwdsN2k0",
        "outputId": "fbdf8f20-78e2-4f24-9c66-308b153eeabd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n",
            "There are 8732 samples in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signal, label = usd[0]"
      ],
      "metadata": {
        "id": "WiY5RP6cT8cp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "Nh6b7GSCclNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn \n",
        "from torchsummary import summary\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 4 conv block -> flatten -> linear -> softmax\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(), \n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(), \n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(), \n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
        "        nn.ReLU(), \n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(128*5*4, 10)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    x = self.conv1(input_data)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear(x)\n",
        "    predictions = self.softmax(logits)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "AKExFh_MZMyV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNNNetwork()\n",
        "summary(cnn.to(device), (1,64,44))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioArWhcTgSQK",
        "outputId": "90afca18-7316-44d7-bc2e-18d6b84a4d57"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "              ReLU-2           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
            "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
            "              ReLU-5           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
            "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
            "              ReLU-8           [-1, 64, 19, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
            "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
            "             ReLU-11           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
            "          Flatten-13                 [-1, 2560]               0\n",
            "           Linear-14                   [-1, 10]          25,610\n",
            "          Softmax-15                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 122,762\n",
            "Trainable params: 122,762\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.83\n",
            "Params size (MB): 0.47\n",
            "Estimated Total Size (MB): 2.31\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "jiA_9CPhh3iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn \n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "BTlbmubJgjnN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(train_data, batch_size):\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "  return train_dataloader \n"
      ],
      "metadata": {
        "id": "ZNZajf1_iLjN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_single_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "  for input, target in data_loader:\n",
        "    input, target = input.to(device), target.to(device)\n",
        "\n",
        "    # calculate loss\n",
        "    prediction = model(input)\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "    # backpropagate error and update weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "\n",
        "  print(f\"loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "Y_njGeiPieSn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
        "  for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}/{epochs}\")\n",
        "    train_single_epoch(model, data_loader, loss_fn, optimizer, device)\n",
        "    print(\"--------------------------------\")\n",
        "  print(\"Finished training\")"
      ],
      "metadata": {
        "id": "JZFou3_CjR1v"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10 \n",
        "LEARNING_RATE = 0.001\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_fft = 1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "ANNOTATIONS_FILE = \"urbansound8k/UrbanSound8K.csv\"\n",
        "AUDIO_DIR = \"urbansound8k\"\n",
        "usd = UrbanSoundDataset(ANNOTATIONS_FILE, \n",
        "                        AUDIO_DIR, \n",
        "                        mel_spectrogram, \n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device)\n",
        "train_dataloader = create_dataloader(usd, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuvWsXInjwQQ",
        "outputId": "0edfa0af-9c09-4831-e2c3-3745439b5047"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb8eqqQpoxUL",
        "outputId": "080c5b3d-39ff-452c-c3f9-b1e12c3dba43"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNNNetwork().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "yUC8jL0Goz95"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTZWIL0_pubg",
        "outputId": "9e773be3-8e1f-4fdf-80b0-398833e70e32"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n",
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(cnn, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n",
        "torch.save(cnn.state_dict(), 'classifier.pth')\n",
        "print(\"Trained Audio classifier and saved at Torch Audio/classifier.pth\")"
      ],
      "metadata": {
        "id": "BUKYyH8QpQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "C4Bb-6Glq8fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 = air_conditioner\n",
        "1 = car_horn\n",
        "2 = children_playing\n",
        "3 = dog_bark\n",
        "4 = drilling\n",
        "5 = engine_idling\n",
        "6 = gun_shot\n",
        "7 = jackhammer\n",
        "8 = siren\n",
        "9 = street_music"
      ],
      "metadata": {
        "id": "ING42Z7fsK3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "class_mapping = [\"air_conditioner\",\"car_horn\",\"children_playing\", \"dog_bark\", \n",
        "                 \"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\",\"siren\",\"street_music\" ]\n",
        "\n",
        "def predict(model, input, target, class_mapping):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions = model(input) \n",
        "    predicted_index = predictions[0].argmax(0)\n",
        "    predicted = class_mapping[predicted_index]\n",
        "    expected = class_mapping[target] \n",
        "  return predicted, expected"
      ],
      "metadata": {
        "id": "AXgpJz53qNfh"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load back the model\n",
        "cnn = CNNNetwork()\n",
        "state_dict = torch.load('classifier.pth')\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "# load urban sound data\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_fft = 1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "ANNOTATIONS_FILE = \"urbansound8k/UrbanSound8K.csv\"\n",
        "AUDIO_DIR = \"urbansound8k\"\n",
        "usd = UrbanSoundDataset(ANNOTATIONS_FILE, \n",
        "                        AUDIO_DIR, \n",
        "                        mel_spectrogram, \n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device)\n"
      ],
      "metadata": {
        "id": "w7JInqAjr76v"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample\n",
        "input, target = usd[5][0], usd[5][1]\n",
        "input.unsqueeze_(0)\n",
        "# inference\n",
        "predicted, expected = predict(cnn, input, target, class_mapping)\n",
        "print(f\"Predicted: {predicted}, Expected: {expected}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFWp7X5FuDIs",
        "outputId": "64ab4801-0c08-4242-d9e2-4ea25f21cab0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: children_playing, Expected: children_playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bBpkOSR4dP6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}